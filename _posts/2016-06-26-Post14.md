---
layout: post
title: Day 24-30 20/6/16-26/6/16
---
In the previous implementation I had used Gradient Descent method for Optimization. It resulted in error% = 13.5% . After that I read about Adam's Optimizer for finding derivatives. Implemented it myself. It reduced the error percentage to 13%. Also, I found out that for Gradient Descent, the learning rate needs to be of the order of 0.001 in my case, for higher values, it starts over-shooting after some number of iterations of learning. While in case of Adam's Optimization method, it does't over-shoot even for values like 0.1 . I went on incrementing values of Alpha, and found that for my case, it works just fine till Alpha=1.5 . But even after increasing the value of Alpha and even varying Lambda (the regularisation parameter to prevent ocer-fitting), all i could achieve was an minimum error % around 13%. For all these variants, it was always around 13-14%. Also, I tried cost functions like- Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) but the results didn't differ much.

Also, one more thing to note is that when I tried using Tensorflow's in-built GradientDescentOptimizer and AdamOptimizer, the computation speed increased 10-fold which seemed unbeieveable to me. After much speculation, all I could conclude was that it's probably because tensorflow's in-built optimizers have C++ implemntation and that's what results in faster processing as is ov=bserved the difference between implemnetation in python and C/C++ .

Anyway, so after some failed attmpts, I read parts of a research paper which did analyis of various state of the art Collaborative filtering algorthms under various conditions (those being size of matrix and how sparse it is).

Another problem that I thought was in my algorithm is that basically after learning, it outputs those movies for a particular user whose predicted ratings are highest. But those ratings were around 8-12, while the range is 0-5. Obviously I couldn't control it to learn within the bounds of 0 to 5, but it should come eventually if the algo learns well. So, what I suspect is that he the movies for which the algo is giving output around 8-12 actally have the most deviation from expected results and seems that the algo hasn't learnt well for those particluar movies for that particular user which is basically against the purpose of collaborative filtering. So, I googled about it and got to read a research paper which showed stastically that though Collaborative Filtering techniques improve the diversity on a person to person basis but decrease the diversity on a mass level, in the sense that the algos generally tend to predict similar for a wide range of users. This problem one usually observes on Quora too nowadays.

Anyway, after feeding my brain with some good food, I returned back to improve efficiency of my algorithm. I started with Geoffrey Hinton's Cousera course on Neural Networks because I got to know that there he talks of Restricted Boltzmann's Machines which if implemented along with traditional collaborative filtering techniques can imporve the efficiency. 

Before I could reach those videos, midway I realised that I hadn't tried varying a parameter in my algorithm yet and that being- number of features. Had set it to 10 by default. So, I tweaked my code a bit which then gave me error % for various values of num_features. So, here we go. 10 features- 13%, 12 features-12.9%, 14 features- 12.2%. I am dumbstruck now. What the hell did just happen. Directly down to 12.2%. Let's go further. 20 features- 10.4 %, 24 features- 9.4%. This is turning really crazy now. From all those efforts going in vain, it's down from 13% to 9.4% now. More excitement. 45 features- 5.07%. Well, shit just got real. 55 features- 3.56% :O , 60 features- 2.9% :O :O , 70 features- 1.9% you are kidding me right. 91 features- 1.00% **just keeps staring**, 102 features- 0.84% **still staring**, 125 features- 0.72%. I can't differentiate between what's real and what's not now , 150 features- 0.68% **surely I am dreaming**, 200 features- 0.664%, 400 features- 0.660%, 800 features-0.659%. 

Well, now that I am slowly coming back to reality, I realize that my system can crash anytime now. I open the system monitor to keep watch on it's performance. Still feeling greedy, I keep on increasing the number of features. Also, I realize that my algo may even start over-shooting now, so, to keep a check, I start printing the error percent after every 10 iterations instead of 100. Uptill now, total iterations were 2500, but I decrease them to 1500. So, here we go- 1000 features. 10 iterations- 109%, 20 iterations- 65.7%, 30-33.5%, 40-20.4%, 50-11.86%, 60-7.02%, 70-4.22%, 80-2.5%, 90-1.5%, 100-0.90%, 110-0.56% what did just happen??? this is seriously crazy. how can it come down to 0.56 in just 110 iterations??? well, I am done with this now ! Next I look back to monitor- 160 iterations- 0.20% whaaat??? let's get slow; 120-0.37%, 130-0.27%, 140-0.23%, 150-0.21%...oh, by now it is 200-0.188%...320-0.17959%, 330- 0.1805%...400-0.197%...500-0.258%...600-0.35%...1000-0.60%...1490-0.650%..aah all this time it was over-shooting. Damn it ! Amyway, let's go forward. 1100 features- minima at 0.1674% after 340 iterations, 1200 features- minima at 0.1552% after 320 iterations. 

Done Man! beyond this, my system will crash with high probability.

Phew !! Well, all this lasted 6-7 hours. After spending some time coming to reality, I again start all this process with total 8000 variants of num_features, Lambda and Alpha in an attempt to know which combination gives best results and leave the system to keep processing it all. When I return, it shows "ValueError: GraphDef cannot be larger than 2GB." Looking forward to get over it...
